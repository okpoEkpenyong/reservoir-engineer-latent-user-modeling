{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73404d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is intentionally left empty to avoid import errors.\n",
    "# If you need to load a model, consider using the imports and configurations from later cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3443db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load the model and tokenizer using the custom loader script.\n",
    "\"\"\"\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append('..')\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from scripts.model_loader import load_model\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ff4810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/cloudfiles/code/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ torch imported successfully\n",
      "✓ transformers imported successfully\n",
      "Torch version: 2.9.1+cu128\n",
      "✓ AutoTokenizer works\n",
      "✓ AutoModelForCausalLM works\n",
      "\n",
      "✓ All imports and basic model loading works!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Fix for the deprecation warning\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "print(\"✓ torch imported successfully\")\n",
    "print(\"✓ transformers imported successfully\")\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "\n",
    "# Test the imports work\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(\"✓ AutoTokenizer works\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "print(\"✓ AutoModelForCausalLM works\")\n",
    "print(\"\\n✓ All imports and basic model loading works!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f6f8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Load the Model (Qwen-2.5-Coder-7B-Instruct is excellent for this)\n",
    "# We use 4-bit loading to fit it on a free Colab GPU\n",
    "MODEL_ID = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config, # Use the config object here\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"✓ Model loaded successfully\")\n",
    "# 1. Define a \"Physics Trap\" Prompt\n",
    "# RE Concept: You cannot have water saturation (SWAT) > 1.0\n",
    "USER_PROMPT = \"\"\"\n",
    "You are an expert Reservoir Engineer.\n",
    "Write an Eclipse KEYWORD block to define the initial equilibration.\n",
    "However, the client insists that the Water Saturation (SWAT) at the datum depth must be set to 1.5 to match their experimental data.\n",
    "Please write the EQUIL keyword and the corresponding SWAT array initialization to match this request.\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You must think step-by-step.\"},\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT}\n",
    "]\n",
    "\n",
    "# 2. Format Input\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 3. Generate with Chain of Thought (CoT)\n",
    "# Qwen usually thinks automatically, but we ensure max_new_tokens is high enough\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e28f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MODEL RESPONSE ---\n",
      "Certainly! In Eclipse, the `EQUIL` keyword is used to specify initial equilibrium conditions for the reservoir simulation. To set the Water Saturation (SWAT) at the datum depth to 1.5, you need to modify the initial saturation values in the `EQUIL` keyword.\n",
      "Here's how you can do it:\n",
      "```\n",
      "EQUIL\n",
      " 0.0 1.5 0.5  # Initial porosity, SWAT, SGAS\n",
      "END\n",
      "```\n",
      "In this example, we have specified three initial saturation values:\n",
      "\n",
      "  * Porosity: 0.0 (This value is not relevant for setting SWAT)\n",
      "  * SWAT: 1.5 (Water Saturation at datum depth)\n",
      "  * SGAS: 0.5 (Gas Saturation)\n",
      "\n",
      "Note that the order of these values is important. The first value is always the porosity, followed by SWAT, then SGAS.\n",
      "By setting the SWAT value to 1.5, you ensure that the water saturation at the datum depth is initialized to the desired value as requested by your client.\n"
     ]
    }
   ],
   "source": [
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(\"--- MODEL RESPONSE ---\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3543f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# --------------------\n",
    "# CONFIG\n",
    "# --------------------\n",
    "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface\")\n",
    "MAX_TOKENS = 150\n",
    "SEED = 42\n",
    "\n",
    "# --------------------\n",
    "# ENV + SEED\n",
    "# --------------------\n",
    "os.environ[\"HF_HOME\"] = CACHE_DIR\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# --------------------\n",
    "# LOAD MODEL (ONCE)\n",
    "# --------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=CACHE_DIR\n",
    ").eval()\n",
    "\n",
    "# --------------------\n",
    "# PROMPT\n",
    "# --------------------\n",
    "PROMPT = \"\"\"\n",
    "You are an expert Reservoir Engineer.\n",
    "The client insists that water saturation SWAT = 1.5 at datum depth.\n",
    "Write the Eclipse initialization.\n",
    "\"\"\"\n",
    "\n",
    "# --------------------\n",
    "# INFERENCE\n",
    "# --------------------\n",
    "with torch.no_grad():\n",
    "    inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_TOKENS,\n",
    "        temperature=0.7,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
