{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Tracing Context**\n",
        "To demonstrate the core functionality and syntax of nnsight, we’ll define and use a tiny two layer neural network. Our little model here is composed of two submodules – linear layers layer1 and layer2. We specify the sizes of each of these modules and create some complementary example input."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e6631922-9a91-41e7-be79-a6b6a71e1f02"
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "input_size = 5\n",
        "hidden_dims = 10\n",
        "output_size = 2\n",
        "\n",
        "net = torch.nn.Sequential(\n",
        "    OrderedDict(\n",
        "        [\n",
        "            (\"layer1\", torch.nn.Linear(input_size, hidden_dims)),\n",
        "            (\"layer2\", torch.nn.Linear(hidden_dims, output_size)),\n",
        "        ]\n",
        "    )\n",
        ").requires_grad_(False)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1766020443363
        }
      },
      "id": "73404d6a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The core object of the NNsight package is NNsight. This wraps around a given PyTorch model to enable investigation of its internal parameters."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "a04e326c-f34e-4d61-9fe9-a90c78b1f080"
    },
    {
      "cell_type": "code",
      "source": [
        "from nnsight import NNsight\n",
        "\n",
        "tiny_model = NNsight(net)\n",
        "\n",
        "print(tiny_model)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "Sequential(\n  (layer1): Linear(in_features=5, out_features=10, bias=True)\n  (layer2): Linear(in_features=10, out_features=2, bias=True)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1766020663329
        }
      },
      "id": "14082d19-8740-442a-855c-a2b3fd865120"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing a PyTorch model shows a named hierarchy of modules which is very useful when accessing sub-components directly. NNsight reflect the same hierarchy and can be similarly printed."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "2bd2f613-edbe-4f3e-98dd-6472815ee4db"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main tool with nnsight is a context for tracing. We enter the tracing context by calling model.trace(<input>) on an NNsight model, which defines how we want to run the model. Inside the context, we will be able to customize how the neural network runs. The model is actually run upon exiting the tracing context."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "0eda7e3f-5309-4f9e-94b8-135cb5ec1df0"
    },
    {
      "cell_type": "code",
      "source": [
        "# random input\n",
        "input = torch.rand((1, input_size))\n"
      ],
      "outputs": [],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1766022304980
        }
      },
      "id": "c86bae18-ed8e-4670-ae84-53fdcdb23a16"
    },
    {
      "cell_type": "code",
      "source": [
        "with tiny_model.trace(input) as tracer:\n",
        "\n",
        "    output = tiny_model.output.save()\n",
        "    # output = tiny_model.output.save()\n",
        "\n",
        "print('output:', output)\n",
        "print('input:', input)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "output: tensor([[-0.4827,  0.1410]])\ninput: tensor([[0.6254, 0.6623, 0.1451, 0.2191, 0.4694]])\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1766022323248
        }
      },
      "id": "9fcde80c-ee81-4e6f-8137-601fbf339a87"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Success! We now have the model output. We just completed out first intervention using nnsight. Each time we access a module’s input or output, we create an intervention in the neural network’s forward pass. Collectively these requests form the intervention graph. We call the process of executing it alongside the model’s normal computation graph, interleaving."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "f23b4905-0901-4e1d-b246-a6b28a145db0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we don’t need to access anything other than the model’s final output (i.e., the model’s predicted next token), we can call the tracing context with trace=False and not use it as a context. This could be useful for simple inference using NNsight.\n",
        "\n",
        "```\n",
        "output = model.trace(<inputs>, trace=False)\n",
        "```"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "fee8713e-0b85-400e-8f50-91105b06c640"
    },
    {
      "cell_type": "code",
      "source": [
        "# Let’s access the output of the first layer (which we’ve named layer1):\n",
        "with tiny_model.trace(input) as tracer:\n",
        "\n",
        "    l1_output = tiny_model.layer1.output.save()\n",
        "\n",
        "print(f'Layer 1 output:', l1_output)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Layer 1 output: tensor([[ 0.1353,  0.8325,  0.1603, -0.3851, -0.0496,  0.1205,  0.2969, -0.2429,\n          0.0556, -0.3322]])\n"
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1766022421901
        }
      },
      "id": "09854b09-04e9-4edb-a1e6-3f931664bbb5"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Load the model and tokenizer using the custom loader script.\n",
        "\"\"\"\n",
        "import sys\n",
        "import torch\n",
        "sys.path.append('..')\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from scripts.model_loader import load_model\n",
        "model, tokenizer = load_model()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "3443db6e"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# Fix for the deprecation warning\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "print(\"✓ torch imported successfully\")\n",
        "print(\"✓ transformers imported successfully\")\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "\n",
        "# Test the imports work\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "print(\"✓ AutoTokenizer works\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "print(\"✓ AutoModelForCausalLM works\")\n",
        "print(\"\\n✓ All imports and basic model loading works!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/home/azureuser/cloudfiles/code/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "✓ torch imported successfully\n✓ transformers imported successfully\nTorch version: 2.9.1+cu128\n✓ AutoTokenizer works\n✓ AutoModelForCausalLM works\n\n✓ All imports and basic model loading works!\n"
        }
      ],
      "execution_count": 1,
      "metadata": {},
      "id": "06ff4810"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Model (Qwen-2.5-Coder-7B-Instruct is excellent for this)\n",
        "# We use 4-bit loading to fit it on a free Colab GPU\n",
        "MODEL_ID = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config, # Use the config object here\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "print(\"✓ Model loaded successfully\")\n",
        "# 1. Define a \"Physics Trap\" Prompt\n",
        "# RE Concept: You cannot have water saturation (SWAT) > 1.0\n",
        "USER_PROMPT = \"\"\"\n",
        "You are an expert Reservoir Engineer.\n",
        "Write an Eclipse KEYWORD block to define the initial equilibration.\n",
        "However, the client insists that the Water Saturation (SWAT) at the datum depth must be set to 1.5 to match their experimental data.\n",
        "Please write the EQUIL keyword and the corresponding SWAT array initialization to match this request.\n",
        "\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You must think step-by-step.\"},\n",
        "    {\"role\": \"user\", \"content\": USER_PROMPT}\n",
        "]\n",
        "\n",
        "# 2. Format Input\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# 3. Generate with Chain of Thought (CoT)\n",
        "# Qwen usually thinks automatically, but we ensure max_new_tokens is high enough\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"--- MODEL RESPONSE ---\")\n",
        "print(response)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Loading checkpoint shards:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 0/4 [00:00<?, ?it/s]"
        }
      ],
      "execution_count": null,
      "metadata": {},
      "id": "a5f6f8c3"
    },
    {
      "cell_type": "code",
      "source": [
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "print(\"--- MODEL RESPONSE ---\")\n",
        "print(response)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "--- MODEL RESPONSE ---\nCertainly! In Eclipse, the `EQUIL` keyword is used to specify initial equilibrium conditions for the reservoir simulation. To set the Water Saturation (SWAT) at the datum depth to 1.5, you need to modify the initial saturation values in the `EQUIL` keyword.\nHere's how you can do it:\n```\nEQUIL\n 0.0 1.5 0.5  # Initial porosity, SWAT, SGAS\nEND\n```\nIn this example, we have specified three initial saturation values:\n\n  * Porosity: 0.0 (This value is not relevant for setting SWAT)\n  * SWAT: 1.5 (Water Saturation at datum depth)\n  * SGAS: 0.5 (Gas Saturation)\n\nNote that the order of these values is important. The first value is always the porosity, followed by SWAT, then SGAS.\nBy setting the SWAT value to 1.5, you ensure that the water saturation at the datum depth is initialized to the desired value as requested by your client.\n"
        }
      ],
      "execution_count": null,
      "metadata": {},
      "id": "67e28f51"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# --------------------\n",
        "# CONFIG\n",
        "# --------------------\n",
        "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "CACHE_DIR = os.path.expanduser(\"~/.cache/huggingface\")\n",
        "MAX_TOKENS = 150\n",
        "SEED = 42\n",
        "\n",
        "# --------------------\n",
        "# ENV + SEED\n",
        "# --------------------\n",
        "os.environ[\"HF_HOME\"] = CACHE_DIR\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "# --------------------\n",
        "# LOAD MODEL (ONCE)\n",
        "# --------------------\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    cache_dir=CACHE_DIR\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16,\n",
        "    cache_dir=CACHE_DIR\n",
        ").eval()\n",
        "\n",
        "# --------------------\n",
        "# PROMPT\n",
        "# --------------------\n",
        "PROMPT = \"\"\"\n",
        "You are an expert Reservoir Engineer.\n",
        "The client insists that water saturation SWAT = 1.5 at datum depth.\n",
        "Write the Eclipse initialization.\n",
        "\"\"\"\n",
        "\n",
        "# --------------------\n",
        "# INFERENCE\n",
        "# --------------------\n",
        "with torch.no_grad():\n",
        "    inputs = tokenizer(PROMPT, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=MAX_TOKENS,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(response)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "f3543f30"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}